{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing\n",
    "import statsmodels.formula.api as smapi\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sale price distribution\n",
    "First step is to look at the target sale price for the training data set, i.e. the column we're trying to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('data/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sale price is in hte hundreds of thousands, so let's divide the price by 1000 to get more manageable numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = target / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(target);\n",
    "plt.title('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "sp.stats.skew(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp.stats.skewtest(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is skewed (as demonstrated by the large z-score (and small pvalue) of teh skewtest). It is right skewed (the skew is positive). Skewed distribution are not ideal for linear models, which often assume a normal distribution. One way to correct for right-skewness is to take the log [1,2]\n",
    "\n",
    "- [1] http://fmwww.bc.edu/repec/bocode/t/transint.html \n",
    "- [2] https://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/\n",
    "- [3] Alexandru Papiu's notebook https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models/commentsnotebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function $x \\rightarrow \\log(1 + x)$ because it is always positive for $x \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logtarget = np.log1p(target)\n",
    "print('skewness of logtarget = ', sp.stats.skew(logtarget)[0])\n",
    "print('skewness test of logtarget = ', sp.stats.skewtest(logtarget))\n",
    "sns.distplot(logtarget)\n",
    "plt.title(r'log(1 + SalePrice)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the training and test datasets for data preparation\n",
    "We're going to explore the training dataset and apply some transformations to it (fixing missing values, transforming columns etc). We'll need to apply the same transformations to the test dataset. To make that easy, let's use a class that maintains the training and test datasets and keeps them in sync (so that when we apply a transformation to the full dataset, it's applied automatically to the training and test datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \"\"\"Helper class to manipulate the training and test datasets seamlessly. \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    df : dataframe\n",
    "         Full data containing both the training and test datasets.\n",
    "    train: dataframe\n",
    "         The training dataset, kept in sync with df.\n",
    "    test: dataframe\n",
    "         The test dataset, kept in sync with df.\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_train, raw_test):\n",
    "        self.raw_train = raw_train\n",
    "        self.raw_test = raw_test\n",
    "        self.train = self.raw_train.copy()\n",
    "        self.test = self.raw_test.copy()\n",
    "        self.df = self.merge(self.raw_train, self.raw_test)\n",
    "        \n",
    "    @staticmethod\n",
    "    def merge(train, test):\n",
    "        return pd.concat([train, test], axis=0, ignore_index=True)\n",
    "    \n",
    "    def split(self, alldf):\n",
    "        n = self.train.shape[0]\n",
    "        train = alldf.iloc[:n, :].set_index(self.raw_train.index)\n",
    "        test = alldf.iloc[n:, :].set_index(self.raw_test.index)\n",
    "        return train, test    \n",
    "    \n",
    "    @property\n",
    "    def df(self):\n",
    "        return self._df\n",
    "    \n",
    "    @df.setter\n",
    "    def df(self, dataframe):\n",
    "        self._df = dataframe\n",
    "        # Update the train and test datasets\n",
    "        self.train, self.test = self.split(self._df)\n",
    "        \n",
    "    def copy(self):\n",
    "        \"\"\"Return a copy of the dataset.\"\"\"\n",
    "        ds = DataSet(self.train, self.test)\n",
    "        ds.raw_train = self.raw_train\n",
    "        ds.raw_test = self.raw_test\n",
    "        return ds\n",
    "    \n",
    "    def apply(self, func, inplace=False):\n",
    "        \"\"\"Apply a function func: dataframe -> dataframe \n",
    "        to the dataset and return the transformed dataset. \n",
    "        Leave raw data unchanged. \n",
    "        \"\"\"\n",
    "        df = func(self.df)\n",
    "        if inplace:\n",
    "            self.df = df\n",
    "            return self\n",
    "        else:\n",
    "            ds = self.copy()\n",
    "            ds.df = df\n",
    "            return ds\n",
    "    \n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\"Try to get the attribute from the the class, \n",
    "        otherwise try to get it from the underlying dataframe.\n",
    "        \"\"\"\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        else:\n",
    "            try:\n",
    "                return self.df.__getattr__(attr)\n",
    "            except AttributeError:\n",
    "                print(\"Unable to find attribute {!r} in self nor in self.df\".format(attr))\n",
    "                raise\n",
    "                        \n",
    "                    \n",
    "raw_train = pd.read_csv('data/train_prepared_light.csv')\n",
    "raw_test = pd.read_csv('data/test_prepared_light.csv')\n",
    "ds = DataSet(raw_train, raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTS\n",
    "def fixture():\n",
    "    return DataSet(raw_train, raw_test)\n",
    "\n",
    "def test_split_merge():\n",
    "    \"\"\"Check the merge and split functions\"\"\"\n",
    "    ds = fixture()\n",
    "    df1, df2 = ds.split(ds.df)\n",
    "    assert all(df1 == raw_train)\n",
    "    assert all(df2 == raw_test)\n",
    "    assert all(ds.merge(df1, df2) == ds.df)\n",
    "\n",
    "    \n",
    "def test_synchronization():\n",
    "    \"\"\"Check that if we update df then the train and test sets are updated accordingly\"\"\"\n",
    "    ds = fixture()\n",
    "    ds.df = 2 * ds.df\n",
    "    assert all(ds.train == 2 * ds.raw_train)\n",
    "    assert all(ds.test == 2 * ds.raw_test)\n",
    "\n",
    "    \n",
    "def test_copy():\n",
    "    ds1 = fixture()\n",
    "    ds2 = ds1.copy()\n",
    "    assert not (ds1 is ds2)\n",
    "    assert all(ds1.df == ds2.df)\n",
    "    assert all(ds1.raw_train == ds2.raw_train)\n",
    "    assert all(ds1.train == ds2.train)\n",
    "    assert all(ds1.test == ds2.test)\n",
    "\n",
    "    \n",
    "def test_apply():\n",
    "    ds = fixture()\n",
    "    ds2 = ds.apply(lambda x: x * 2)\n",
    "    assert not (ds is ds2)\n",
    "    assert all(ds.df == ds2.df * 2)\n",
    "\n",
    "    \n",
    "def test_apply_inplace():\n",
    "    ds = fixture()\n",
    "    ds_init = ds.copy()\n",
    "    ds2 = ds.apply(lambda x: x * 2, inplace=True)\n",
    "    assert (ds is ds2)\n",
    "    assert all(ds2.df == ds_init.df * 2)\n",
    "    assert all(ds2.raw_train == ds_init.raw_train)\n",
    "\n",
    "def test_getattr():\n",
    "    \"\"\"Get an attribute of the underlying dataframe if possible\"\"\"\n",
    "    ds = fixture()\n",
    "    assert all(ds.columns == ds.df.columns)\n",
    "    assert ds.shape == ds.df.shape\n",
    "    \n",
    "test_split_merge()\n",
    "test_synchronization()\n",
    "test_copy()\n",
    "test_apply()\n",
    "test_apply_inplace()\n",
    "test_getattr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncategories = sum(ds.df.dtypes == object)\n",
    "ncategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "The dataset is wide with 78 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds.columns, len(ds.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got 3 data types: int, float and object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds.df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data between categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_categorical = (ds.df.dtypes == object)\n",
    "is_numerical = (~ is_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numerical dataset to keep track of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum = ds.apply(lambda df: df.loc[:, is_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum.columns, len(dsnum.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got 36 numerical features. We can use the `describe` method to get some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's a lot of numbers to digest. Better get started plotting! To help with plotting, but also to improve linear regression models, we're going to standardize our data. But before that we must deal with the NaN values.\n",
    "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with NaN values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum = dsnum.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_with_nulls = dfnum.columns[dfnum.isnull().sum() > 0]\n",
    "cols_with_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum[cols_with_nulls].isnull().sum().sort_values(ascending=False)\n",
    "#.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the description, the null values for the `MasVnrArea` should be 0 (no massonry veneer type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We may want to refine this in the future. Perhaps build a model to predict the missing GarageCars from the other features?\n",
    "median_list = 'LotFrontage', 'BsmtFullBath','BsmtHalfBath', 'GarageCars', 'GarageArea'\n",
    "zero_list = 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', 'BsmtUnfSF'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feature in median_list:\n",
    "    dfnum[feature].fillna(dfnum[feature].median(), inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feature in zero_list:\n",
    "    dfnum[feature].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the GarageYrBlt, replace by the year the house was built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfnum.GarageYrBlt.fillna(dfnum.YearBuilt[dfnum.GarageYrBlt.isnull()], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum.df = dfnum\n",
    "\n",
    "# Check that everything is in order\n",
    "def has_nulls(df):\n",
    "    return df.isnull().sum().any()\n",
    "\n",
    "assert not has_nulls(dfnum)\n",
    "assert not has_nulls(dsnum.df)\n",
    "assert not has_nulls(dsnum.train)\n",
    "assert not has_nulls(dsnum.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    _values = sk.preprocessing.StandardScaler().fit_transform(df)\n",
    "    return pd.DataFrame(data=_values, columns=df.columns)\n",
    "\n",
    "dsnum_t = dsnum.apply(standardize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot violinplots for each feature \n",
    "The violin plots give us some idea of the distribution of data for each feature. We can look for things like skewness, non-normality, and the presence of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def violinplot(df, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    sns.violinplot(df, ax=ax)\n",
    "    for xlab in ax.get_xticklabels():\n",
    "        xlab.set_rotation(30)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featureplot(df, nrows=1, figsize=(12,8), plotfunc=violinplot):\n",
    "    \"\"\"Plot the dataframe features\"\"\"\n",
    "    width, height = figsize\n",
    "    fig, axes = plt.subplots(nrows, 1, figsize=(width, height * nrows));\n",
    "    i = 0\n",
    "    plots_per_figure = df.shape[1] // nrows\n",
    "    if nrows == 1:\n",
    "        axes = [axes]\n",
    "    for j, ax in zip(range(plots_per_figure, df.shape[1] + 1, plots_per_figure), axes):\n",
    "        plotfunc(df.iloc[:, i:j], ax=ax)\n",
    "        i = j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum_t.train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the features are higly skewed with very long tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureplot(dsnum_t.train.iloc[:, 0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these are right skewed as well. BsmtFullBath has some discrete values (number of bathrooms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureplot(dsnum_t.train.iloc[:, 9:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features, such as `BsmtFinSF2`, are almost constant (blobs with long tail) as can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(4, 4))\n",
    "sns.distplot(dsnum_t.train['BsmtFinSF2'], ax=ax)\n",
    "ax.set_title('Distribution of BsmtFinSF2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop nearly constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_nearly_constant(series):\n",
    "    counts = series.value_counts()\n",
    "    max_val_count = max(counts)\n",
    "    other_val_count = counts.drop(counts.argmax()).sum()\n",
    "    return other_val_count / max_val_count < 0.25\n",
    "\n",
    "is_nearly_constant = dsnum_t.train.apply(test_nearly_constant)\n",
    "is_nearly_constant.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropme = dsnum_t.columns[is_nearly_constant]\n",
    "dropme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to drop these nearly constant features. If we want to have more control we can transform them into categorical features (for example, is there a screen porch or not?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum_t.columns, dsnum_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum_t.df = dsnum_t.df.drop(dropme, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum_t.columns, dsnum_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transform the other features if they have a high skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a log transformation for some of the skewed features should help, as illustrated below. We use the raw data (not the standardized one) because we need positive values for the log function (we'll standardize the transformed variables later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(8, 4))\n",
    "sns.distplot(dsnum.train['LotArea'], ax=axes[0])\n",
    "sns.distplot(np.log1p(dsnum.train['LotArea']), ax=axes[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zfactors = sp.stats.skewtest(dsnum_t.train)[0]\n",
    "sns.distplot(zfactors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_skewed = np.abs(zfactors) > 10\n",
    "pd.Series(data=zfactors, index=dsnum_t.df.columns)[is_skewed].sort_values().plot(kind='barh')\n",
    "plt.title('Z-factor for skewtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the sign of the skewness for all these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert all(np.sign(sp.stats.skew(dfnum_t)[is_skewed]) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a log1p transform to all these and plot the distributions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_skewed_colums(dfnum, dropme=dropme, is_skewed=is_skewed):\n",
    "    \"\"\"\n",
    "    dfnum: dataframe to transform\n",
    "    dropme: columns to drop\n",
    "    is_skewed: iterable of length dfnum.columns indicating if a column is skewed\n",
    "    \"\"\"\n",
    "    dfnum2 = dfnum.copy()\n",
    "    for feature, skewed_feature in zip(dfnum.columns, is_skewed):\n",
    "        if skewed_feature:\n",
    "            dfnum2[feature] = np.log1p(dfnum[feature])\n",
    "\n",
    "    dfnum_t2 = standardize(dfnum2).drop(dropme, axis=1)\n",
    "    return dfnum_t2\n",
    "\n",
    "# the transformed dataset has fewer columns and we only want those\n",
    "dsnum_t2 = dsnum.apply(transform_skewed_colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum_t2.df.iloc[:, is_skewed].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zfactors2 = sp.stats.skewtest(dsnum_t2.train)[0]\n",
    "pd.Series(data=zfactors2, index=dsnum_t2.columns)[is_skewed].sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our originally skewed features look more symmetric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureplot(dsnum_t2.train.iloc[:, is_skewed], nrows=2, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureplot(dsnum_t2.train.iloc[:, ~is_skewed], nrows=2, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "We're now in a good position to identify the key numerical features. Those should be hightly correlated with the sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfeatures = dsnum_t2.columns\n",
    "target_t = standardize(logtarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = pd.DataFrame(dsnum_t2.train.apply(lambda feature: sp.stats.pearsonr(feature, target_t['SalePrice'])),\n",
    "                   columns=['pearsonr'])\n",
    "corr['correlation'] = corr['pearsonr'].apply(lambda x: x[0])\n",
    "corr['pvalue'] = corr['pearsonr'].apply(lambda x: x[1])\n",
    "corr.drop('pearsonr', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr.sort_values('pvalue', ascending=False)['correlation'].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr.sort_values('pvalue').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr.sort_values('pvalue').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep only the features that have a high enough correlation with the price (correlation less than 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corr.filter?\n",
    "min_correlation = 0.2\n",
    "key_features = corr[np.abs(corr['correlation'] > min_correlation)].sort_values(by='correlation', ascending=False).index.values\n",
    "key_features, key_features.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic linear regression model\n",
    "We're left with 22 features. The first 4 should all be highly correlated with the price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = dsnum_t2.train.copy()\n",
    "data['SalePrice'] = target_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(10,10))\n",
    "for feature, ax in zip(key_features[:4], itertools.chain.from_iterable(axes)):\n",
    "    ax.plot(data[feature], data['SalePrice'], 'o')\n",
    "    ax.set(xlabel=feature, ylabel='SalePrice')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a simple linear regression model based on these 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression1 = smapi.ols(\"SalePrice ~ OverallQual + GrLivArea + GarageCars + GarageArea\", data=data).fit()\n",
    "regression1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared equals 0.79 so it's pretty good for a first try. Let's see what happens if we include all our numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels gets confused with columns that start with a digit, so let's rename that column first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['1stFlrSF'].name = 'FlrSF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.rename_axis({'1stFlrSF': 'FirstFlrSF', '2ndFlrSF': 'SndFlrSF'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc = 'SalePrice ~ ' + ' + '.join(data.drop('SalePrice', axis=1))\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen below, using more numerical values improves R-squared to 0.88 which is pretty good, though there's of course a risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression2 = smapi.ols(desc, data=data).fit()\n",
    "regression2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data(X, y):\n",
    "    df = X.copy()\n",
    "    df['SalePrice'] = y\n",
    "    return df\n",
    "\n",
    "def ols1(X, y):\n",
    "    data = get_data(X, y)\n",
    "    return smapi.ols(\"SalePrice ~ OverallQual + GrLivArea + GarageCars + GarageArea\", data=data)\n",
    "\n",
    "def ols2(X, y):\n",
    "    data = get_data(X, y)\n",
    "    return smapi.ols(desc, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model \n",
    "### Use `sklearn.model_selection.train_test_split` to run some experiments and validate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def rmse(prediction, exact):\n",
    "    return np.mean((prediction - exact)**2.0)**0.5\n",
    "\n",
    "def run_experiment(estimator, scoring=rmse):\n",
    "    Xtrain, Xtest, ytrain, ytest = sk.model_selection.train_test_split(data.drop('SalePrice', axis=1), data['SalePrice'])\n",
    "    model = estimator(Xtrain, ytrain).fit()\n",
    "    return scoring(model.predict(Xtest), ytest)\n",
    "\n",
    "def cross_validate(estimator, cv=5):\n",
    "    return np.array([run_experiment(estimator) for _ in range(cv)])\n",
    "\n",
    "for model in [ols1, ols2]:\n",
    "    errors = cross_validate(model)\n",
    "    print(errors, errors.mean())\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `sklearn.model_selection_cross_val_score` to validate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Regressor(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self.estimator(X, y).fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "\n",
    "for model in [ols1, ols2]:\n",
    "    mse = np.sqrt(-sk.model_selection.cross_val_score(Regressor(model), data.drop('SalePrice', axis=1), y=data['SalePrice'],  \n",
    "                                   scoring='neg_mean_squared_error', cv=5))\n",
    "    print(mse, mse.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsnum_t2_submission = dsnum_t2.apply(lambda df: df.rename_axis({'1stFlrSF': 'FirstFlrSF', '2ndFlrSF': 'SndFlrSF'}, axis=1))\n",
    "submission_t = regression2.predict(dsnum_t2_submission.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inverse_transform_target(target_t):\n",
    "    scaler = sk.preprocessing.StandardScaler()\n",
    "    scaler.fit(logtarget)\n",
    "    log_submission = scaler.inverse_transform(submission_t)\n",
    "    return np.expm1(log_submission) * 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = inverse_transform_target(submission_t)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save(filename, submission):\n",
    "    df = pd.DataFrame(data={\n",
    "            \"Id\": np.arange(len(submission)) + 1461,\n",
    "            \"SalePrice\": submission\n",
    "            })\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "save('ols_key_numerical_features_only.csv', submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression interpretation\n",
    "Statsmodels has special plots to explore the outcome of a regression model\n",
    "http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/example_regression_plots.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
